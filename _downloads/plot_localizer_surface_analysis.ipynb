{
  "nbformat_minor": 0, 
  "nbformat": 4, 
  "cells": [
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "%matplotlib inline"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Example of surface-based first-level analysis\n=============================================\n\nFull step-by-step example of fitting a GLM to experimental data\nsampled on the cortical surface and visualizing the results.\n\nMore specifically:\n\n1. A sequence of fMRI volumes are loaded\n2. fMRI data are projected onto a reference cortical surface (the\nfreesurfer template, fsaverage)\n3. A design matrix describing all the effects related to the data is computed\n4. A GLM is applied to the dataset (effect/covariance,\n   then contrast estimation)\n\nThe result of the analysis are statistical maps that are defined on\nthe brain mesh. We display them using Nilearn capabilities.\n\nThe projection of fMRI data onto a given brain mesh requires that both\nare initially defined in the same space.\n\n* The functional data should be coregistered to the anatomy from which\n  the mesh was obtained.\n\n* Another possibility, used here, is to project the normalized fMRI\n  data to an MNI-coregistered mesh, such as fsaverage.\n\nThe advantage of this second approach is that it makes it easy to run\nsecond-level analyses on the surface. On the other hand, it is\nobviously less accurate than using a subject-tailored mesh.\n\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Prepare data and analysis parameters\n-------------------------------------\nPrepare timing parameters\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "t_r = 2.4\nslice_time_ref = 0.5"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Prepare data\nFirst the volume-based fMRI data.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from nistats.datasets import fetch_localizer_first_level\ndata = fetch_localizer_first_level()\nfmri_img = data.epi_img"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Second the experimental paradigm.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "events_file = data['events']\nimport pandas as pd\nevents = pd.read_table(events_file)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Project the fMRI image to the surface\n-------------------------------------\n\nFor this we need to get a mesh representing the geometry of the\nsurface.  we could use an individual mesh, but we first resort to a\nstandard mesh, the so-called fsaverage5 template from the Freesurfer\nsoftware.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "import nilearn\nfsaverage = nilearn.datasets.fetch_surf_fsaverage5()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "The projection function simply takes the fMRI data and the mesh.\nNote that those correspond spatially, are they are bothin MNI space.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from nilearn import surface\ntexture = surface.vol_to_surf(fmri_img, fsaverage.pial_right)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Perform first level analysis\n----------------------------\n\nThis involves computing the design matrix and fitting the model.\nWe start by specifying the timing of fMRI frames\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "import numpy as np\nn_scans = texture.shape[1]\nframe_times = t_r * (np.arange(n_scans) + .5)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Create the design matrix\n\nWe specify an hrf model containing Glover model and its time derivative\nthe drift model is implicitly a cosine basis with period cutoff 128s.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from nistats.design_matrix import make_first_level_design_matrix\ndesign_matrix = make_first_level_design_matrix(frame_times,\n                                               events=events,\n                                               hrf_model='glover + derivative'\n                                               )"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Setup and fit GLM.\nNote that the output consists in 2 variables: `labels` and `fit`\n`labels` tags voxels according to noise autocorrelation.\n`estimates` contains the parameter estimates.\nWe keep them for later contrast computation.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from nistats.first_level_model import run_glm\nlabels, estimates = run_glm(texture.T, design_matrix.values)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Estimate contrasts\n------------------\nSpecify the contrasts\nFor practical purpose, we first generate an identity matrix whose size is\nthe number of columns of the design matrix\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "contrast_matrix = np.eye(design_matrix.shape[1])"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "first create basic contrasts\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "basic_contrasts = dict([(column, contrast_matrix[i])\n                        for i, column in enumerate(design_matrix.columns)])"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "add some intermediate contrasts\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "basic_contrasts[\"audio\"] = (basic_contrasts[\"clicDaudio\"]\n                            + basic_contrasts[\"clicGaudio\"]\n                            + basic_contrasts[\"calculaudio\"]\n                            + basic_contrasts[\"phraseaudio\"]\n                            )\nbasic_contrasts[\"video\"] = (basic_contrasts[\"clicDvideo\"]\n                            + basic_contrasts[\"clicGvideo\"]\n                            + basic_contrasts[\"calculvideo\"]\n                            + basic_contrasts[\"phrasevideo\"]\n                            )\nbasic_contrasts[\"computation\"] = (basic_contrasts[\"calculaudio\"]\n                                  + basic_contrasts[\"calculvideo\"]\n                                  )\nbasic_contrasts[\"sentences\"] = (basic_contrasts[\"phraseaudio\"]\n                                + basic_contrasts[\"phrasevideo\"]\n                                )"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Finally make a dictionary of more relevant contrasts\n\n* \"left - right button press\" probes motor activity in left versus right button presses\n* \"audio - video\" probes the difference of activity between listening to some content or reading the same type of content (instructions, stories)\n* \"computation - sentences\" looks at the activity when performing a mental comptation task  versus simply reading sentences.\n\nOf course, we could define other contrasts, but we keep only 3 for simplicity.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "contrasts = {\n    \"left - right button press\": (basic_contrasts[\"clicGaudio\"]\n                                  + basic_contrasts[\"clicGvideo\"]\n                                  - basic_contrasts[\"clicDaudio\"]\n                                  - basic_contrasts[\"clicDvideo\"]\n                                  ),\n    \"audio - video\": basic_contrasts[\"audio\"] - basic_contrasts[\"video\"],\n    \"computation - sentences\": (basic_contrasts[\"computation\"] -\n                                basic_contrasts[\"sentences\"]\n                                )\n    }"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "contrast estimation\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from nistats.contrasts import compute_contrast\nfrom nilearn import plotting"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "iterate over contrasts\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "for index, (contrast_id, contrast_val) in enumerate(contrasts.items()):\n    print('  Contrast % i out of %i: %s, right hemisphere' %\n          (index + 1, len(contrasts), contrast_id))\n    # compute contrast-related statistics\n    contrast = compute_contrast(labels, estimates, contrast_val,\n                                contrast_type='t')\n    # we present the Z-transform of the t map\n    z_score = contrast.z_score()\n    # we plot it on the surface, on the inflated fsaverage mesh,\n    # together with a suitable background to give an impression\n    # of the cortex folding.\n    plotting.plot_surf_stat_map(\n        fsaverage.infl_right, z_score, hemi='right',\n        title=contrast_id, colorbar=True,\n        threshold=3., bg_map=fsaverage.sulc_right)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Analysing the left hemisphere\n-----------------------------\n\nNote that it requires little additional code!\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Project the fMRI data to the mesh\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "texture = surface.vol_to_surf(fmri_img, fsaverage.pial_left)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Estimate the General Linear Model\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "labels, estimates = run_glm(texture.T, design_matrix.values)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Create contrast-specific maps\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "for index, (contrast_id, contrast_val) in enumerate(contrasts.items()):\n    print('  Contrast % i out of %i: %s, left hemisphere' %\n          (index + 1, len(contrasts), contrast_id))\n    # compute contrasts\n    contrast = compute_contrast(labels, estimates, contrast_val,\n                                contrast_type='t')\n    z_score = contrast.z_score()\n    # Plot the result\n    plotting.plot_surf_stat_map(\n        fsaverage.infl_left, z_score, hemi='left',\n        title=contrast_id, colorbar=True,\n        threshold=3., bg_map=fsaverage.sulc_left)\n\nplotting.show()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2", 
      "name": "python2", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "2.7.15", 
      "pygments_lexer": "ipython2", 
      "codemirror_mode": {
        "version": 2, 
        "name": "ipython"
      }
    }
  }
}