{
  "nbformat_minor": 0, 
  "nbformat": 4, 
  "cells": [
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "%matplotlib inline"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "\nFirst level analysis of localizer dataset\n=========================================\n\nFull step-by-step example of fitting a GLM to experimental data and visualizing\nthe results.\n\nMore specifically:\n\n1. A sequence of fMRI volumes are loaded\n2. A design matrix describing all the effects related to the data is computed\n3. a mask of the useful brain volume is computed\n4. A GLM is applied to the dataset (effect/covariance,\n   then contrast estimation)\n\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from os import mkdir, path\n\nimport numpy as np\nimport pandas as pd\nfrom nilearn import plotting\n\nfrom nistats.first_level_model import FirstLevelModel\nfrom nistats import datasets"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Prepare data and analysis parameters\n-------------------------------------\nPrepare timing\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "t_r = 2.4\nslice_time_ref = 0.5\n\n# Prepare data\ndata = datasets.fetch_localizer_first_level()\nparadigm_file = data.paradigm\nparadigm = pd.read_csv(paradigm_file, sep=' ', header=None, index_col=None)\nparadigm.columns = ['session', 'trial_type', 'onset']\nfmri_img = data.epi_img"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Perform first level analysis\n----------------------------\nSetup and fit GLM\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "first_level_model = FirstLevelModel(t_r, slice_time_ref,\n                                    hrf_model='glover + derivative')\nfirst_level_model = first_level_model.fit(fmri_img, paradigm)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Estimate contrasts\n------------------\nSpecify the contrasts\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "design_matrix = first_level_model.design_matrices_[0]\ncontrast_matrix = np.eye(design_matrix.shape[1])\ncontrasts = dict([(column, contrast_matrix[i])\n                  for i, column in enumerate(design_matrix.columns)])\n\ncontrasts[\"audio\"] = contrasts[\"clicDaudio\"] + contrasts[\"clicGaudio\"] +\\\n    contrasts[\"calculaudio\"] + contrasts[\"phraseaudio\"]\ncontrasts[\"video\"] = contrasts[\"clicDvideo\"] + contrasts[\"clicGvideo\"] + \\\n    contrasts[\"calculvideo\"] + contrasts[\"phrasevideo\"]\ncontrasts[\"computation\"] = contrasts[\"calculaudio\"] + contrasts[\"calculvideo\"]\ncontrasts[\"sentences\"] = contrasts[\"phraseaudio\"] + contrasts[\"phrasevideo\"]"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Short list of more relevant contrasts\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "contrasts = {\n    \"left-right\": (contrasts[\"clicGaudio\"] + contrasts[\"clicGvideo\"]\n                   - contrasts[\"clicDaudio\"] - contrasts[\"clicDvideo\"]),\n    \"H-V\": contrasts[\"damier_H\"] - contrasts[\"damier_V\"],\n    \"audio-video\": contrasts[\"audio\"] - contrasts[\"video\"],\n    \"video-audio\": -contrasts[\"audio\"] + contrasts[\"video\"],\n    \"computation-sentences\": (contrasts[\"computation\"] -\n                              contrasts[\"sentences\"]),\n    \"reading-visual\": contrasts[\"phrasevideo\"] - contrasts[\"damier_H\"]\n    }"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "contrast estimation\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "for index, (contrast_id, contrast_val) in enumerate(contrasts.items()):\n    print('  Contrast % 2i out of %i: %s' %\n          (index + 1, len(contrasts), contrast_id))\n    z_map = first_level_model.compute_contrast(contrast_val,\n                                               output_type='z_score')\n\n    # Create snapshots of the contrasts\n    display = plotting.plot_stat_map(z_map, display_mode='z',\n                                     threshold=3.0, title=contrast_id)\n\nplotting.show()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2", 
      "name": "python2", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "2.7.12", 
      "pygments_lexer": "ipython2", 
      "codemirror_mode": {
        "version": 2, 
        "name": "ipython"
      }
    }
  }
}