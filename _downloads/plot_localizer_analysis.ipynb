{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nFirst level analysis of localizer dataset\n=========================================\n\nFull step-by-step example of fitting a GLM to experimental data and visualizing\nthe results.\n\nMore specifically:\n\n1. A sequence of fMRI volumes are loaded\n2. A design matrix describing all the effects related to the data is computed\n3. a mask of the useful brain volume is computed\n4. A GLM is applied to the dataset (effect/covariance,\n   then contrast estimation)\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from os import mkdir, path\n\nimport numpy as np\nimport pandas as pd\nfrom nilearn import plotting\n\nfrom nistats.first_level_model import FirstLevelModel\nfrom nistats import datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prepare data and analysis parameters\n-------------------------------------\nPrepare timing\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "t_r = 2.4\nslice_time_ref = 0.5\n\n# Prepare data\ndata = datasets.fetch_localizer_first_level()\nparadigm_file = data.paradigm\nparadigm = pd.read_csv(paradigm_file, sep=' ', header=None, index_col=None)\nparadigm.columns = ['session', 'trial_type', 'onset']\nfmri_img = data.epi_img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Perform first level analysis\n----------------------------\nSetup and fit GLM\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "first_level_model = FirstLevelModel(t_r, slice_time_ref,\n                                    hrf_model='glover + derivative')\nfirst_level_model = first_level_model.fit(fmri_img, paradigm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Estimate contrasts\n------------------\nSpecify the contrasts\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "design_matrix = first_level_model.design_matrices_[0]\ncontrast_matrix = np.eye(design_matrix.shape[1])\ncontrasts = dict([(column, contrast_matrix[i])\n                  for i, column in enumerate(design_matrix.columns)])\n\ncontrasts[\"audio\"] = contrasts[\"clicDaudio\"] + contrasts[\"clicGaudio\"] +\\\n    contrasts[\"calculaudio\"] + contrasts[\"phraseaudio\"]\ncontrasts[\"video\"] = contrasts[\"clicDvideo\"] + contrasts[\"clicGvideo\"] + \\\n    contrasts[\"calculvideo\"] + contrasts[\"phrasevideo\"]\ncontrasts[\"computation\"] = contrasts[\"calculaudio\"] + contrasts[\"calculvideo\"]\ncontrasts[\"sentences\"] = contrasts[\"phraseaudio\"] + contrasts[\"phrasevideo\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Short list of more relevant contrasts\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "contrasts = {\n    \"left-right\": (contrasts[\"clicGaudio\"] + contrasts[\"clicGvideo\"]\n                   - contrasts[\"clicDaudio\"] - contrasts[\"clicDvideo\"]),\n    \"H-V\": contrasts[\"damier_H\"] - contrasts[\"damier_V\"],\n    \"audio-video\": contrasts[\"audio\"] - contrasts[\"video\"],\n    \"video-audio\": -contrasts[\"audio\"] + contrasts[\"video\"],\n    \"computation-sentences\": (contrasts[\"computation\"] -\n                              contrasts[\"sentences\"]),\n    \"reading-visual\": contrasts[\"phrasevideo\"] - contrasts[\"damier_H\"]\n    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "contrast estimation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for index, (contrast_id, contrast_val) in enumerate(contrasts.items()):\n    print('  Contrast % 2i out of %i: %s' %\n          (index + 1, len(contrasts), contrast_id))\n    z_map = first_level_model.compute_contrast(contrast_val,\n                                               output_type='z_score')\n\n    # Create snapshots of the contrasts\n    display = plotting.plot_stat_map(z_map, display_mode='z',\n                                     threshold=3.0, title=contrast_id)\n\nplotting.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}