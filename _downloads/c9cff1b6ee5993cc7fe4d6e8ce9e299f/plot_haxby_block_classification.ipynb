{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nDecoding of a dataset after glm fit for signal extraction\n=========================================================\n\nFull step-by-step example of fitting a GLM to perform a decoding experiment.\nWe use the data from one subject of the Haxby dataset.\n\nMore specifically:\n\n1. Download the Haxby dataset.\n2. Extract the information to generate a glm representing the blocks of stimuli.\n3. Analyze the decoding performance using a classifier.\n\nTo run this example, you must launch IPython via ``ipython\n--matplotlib`` in a terminal, or use the Jupyter notebook.\n    :depth: 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fetch example Haxby dataset\n----------------------------\nWe download the Haxby dataset\nThis is a study of visual object category representation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn import datasets\n\n# By default 2nd subject will be fetched\nhaxby_dataset = datasets.fetch_haxby()\n\n# repetition has to be known\nTR = 2.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the behavioral data\n-------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n\n# Load target information as string and give a numerical identifier to each\nbehavioral = pd.read_csv(haxby_dataset.session_target[0], sep=' ')\nconditions = behavioral['labels'].values\n\n# Record these as an array of sessions\nsessions = behavioral['chunks'].values\nunique_sessions = behavioral['chunks'].unique()\n\n# fMRI data: a unique file for each session\nfunc_filename = haxby_dataset.func[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Build a proper event structure for each session\n-----------------------------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nevents = {}\n# events will take  the form of a dictionary of Dataframes, one per session\nfor session in unique_sessions:\n    # get the condition label per session\n    conditions_session = conditions[sessions == session]\n    # get the number of scans per session, then the corresponding\n    # vector of frame times\n    n_scans = len(conditions_session)\n    frame_times = TR * np.arange(n_scans)\n    # each event last the full TR\n    duration = TR * np.ones(n_scans)\n    # Define the events object\n    events_ = pd.DataFrame(\n        {'onset': frame_times, 'trial_type': conditions_session, 'duration': duration})\n    # remove the rest condition and insert into the dictionary\n    events[session] = events_[events_.trial_type != 'rest']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Instantiate and run FirstLevelModel\n-----------------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.image import index_img\nfrom nistats.first_level_model import FirstLevelModel\n\n# we are going to generate a list of z-maps together with their session and condition index\nz_maps = []\ncondition_idx = []\nsession_idx = []\n\n# Instantiate the glm\nglm = FirstLevelModel(t_r=TR,\n                      mask=haxby_dataset.mask,\n                      high_pass=.008,\n                      smoothing_fwhm=4,\n                      memory='nilearn_cache')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the glm on data from each session\n-------------------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for session in unique_sessions:\n    # grab the fmri data for that particular session\n    fmri_session = index_img(func_filename, sessions == session)\n\n    # fit the glm\n    glm.fit(fmri_session, events=events[session])\n\n    # set up contrasts: one per condition\n    conditions = events[session].trial_type.unique()\n    for condition_ in conditions:\n        z_maps.append(glm.compute_contrast(condition_))\n        condition_idx.append(condition_)\n        session_idx.append(session)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generating a report\n-------------------\nSince we have already computed the FirstLevelModel\nand have the contrast, we can quickly create a summary report.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.image import mean_img\nfrom nistats.reporting import make_glm_report\n\nmean_img_ = mean_img(func_filename)\nreport = make_glm_report(glm,\n                         contrasts=conditions,\n                         bg_img=mean_img_,\n                         )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In a jupyter notebook, the report will be automatically inserted, as above.\nWe have several other ways to access the report:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# report  # This report can be viewed in a notebook\n# report.save_as_html('report.html')\n# report.open_in_browser()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Transform the maps to an array of values\n----------------------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.input_data import NiftiMasker\n\n# no need to standardize or smooth the data\nmasker = NiftiMasker(mask_img=haxby_dataset.mask, memory='nilearn_cache',\n                     memory_level=1)\nX = masker.fit_transform(z_maps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Build the decoder\n------------------\nDefine the prediction function to be used.\nHere we use a Support Vector Classification, with a linear kernel\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n\nsvc = SVC(kernel='linear')\n\n# Define the dimension reduction to be used.\n# Here we use a classical univariate feature selection based on F-test,\n# namely Anova. When doing full-brain analysis, it is better to use\n# SelectPercentile, keeping 5% of voxels\n# (because it is independent of the resolution of the data).\nfrom sklearn.feature_selection import SelectPercentile, f_classif\n\nfeature_selection = SelectPercentile(f_classif, percentile=5)\n\n# We have our classifier (SVC), our feature selection (SelectPercentile),and now,\n# we can plug them together in a *pipeline** that performs the two operations\n# successively:\nfrom sklearn.pipeline import Pipeline\n\nanova_svc = Pipeline([('anova', feature_selection), ('svc', svc)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Obtain prediction scores via cross validation\n-----------------------------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score, LeaveOneGroupOut\n\n# Define the cross-validation scheme used for validation.\n# Here we use a LeaveOneGroupOut cross-validation on the session group\n# which corresponds to a leave-one-session-out\ncv = LeaveOneGroupOut()\n\n# Compute the prediction accuracy for the different folds (i.e. session)\ncv_scores = cross_val_score(anova_svc, X, condition_idx, cv=cv, groups=session_idx)\n\n# Return the corresponding mean prediction accuracy\nclassification_accuracy = cv_scores.mean()\nchance_level = 1. / len(np.unique(condition_idx))\n\n# Print the results\nprint('Classification accuracy: {:.4f} / Chance level: {}'.format(\n        classification_accuracy, chance_level))\n# Classification accuracy:  0.375 / Chance level: 0.125"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}