

.. _sphx_glr_auto_examples_01_tutorials_plot_spm_auditory.py:


Univariate analysis of block design, one condition versus rest, single subject
==============================================================================

In this tutorial, we compare the fMRI signal during periods of auditory
stimulation versus periods of rest, using a General Linear Model (GLM). We will
use a univariate approach in which independent tests are performed at
each single-voxel.

The dataset comes from experiment conducted at the FIL by Geriant Rees
under the direction of Karl Friston. It is provided by FIL methods
group which develops the SPM software.

According to SPM documentation, 96 acquisitions were made (RT=7s), in
blocks of 6, giving 16 42s blocks. The condition for successive blocks
alternated between rest and auditory stimulation, starting with rest.
Auditory stimulation was bi-syllabic words presented binaurally at a
rate of 60 per minute. The functional data starts at acquisiton 4,
image fM00223_004.

The whole brain BOLD/EPI images were acquired on a modified 2T Siemens
MAGNETOM Vision system. Each acquisition consisted of 64 contiguous
slices (64x64x64 3mm x 3mm x 3mm voxels). Acquisition took 6.05s, with
the scan to scan repeat time (RT) set arbitrarily to 7s.


This analyse described here is performed in the native space, on the
original EPI scans without any spatial or temporal preprocessing.
(More sensitive results would likely be obtained on the corrected,
spatially normalized and smoothed images).


To run this example, you must launch IPython via ``ipython
--matplotlib`` in a terminal, or use the Jupyter notebook.

.. contents:: **Contents**
    :local:
    :depth: 1




.. code-block:: python


    import matplotlib.pyplot as plt







Retrieving the data
-------------------

.. note:: In this tutorial, we load the data using a data downloading
          function.To input your own data, you will need to pass
          a list of paths to your own files.



.. code-block:: python


    from nistats.datasets import fetch_spm_auditory
    subject_data = fetch_spm_auditory()








We can list the filenames of the functional images



.. code-block:: python

    print(subject_data.func)





.. rst-class:: sphx-glr-script-out

 Out::

    ['/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_004.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_005.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_006.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_007.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_008.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_009.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_010.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_011.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_012.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_013.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_014.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_015.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_016.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_017.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_018.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_019.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_020.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_021.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_022.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_023.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_024.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_025.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_026.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_027.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_028.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_029.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_030.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_031.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_032.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_033.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_034.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_035.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_036.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_037.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_038.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_039.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_040.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_041.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_042.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_043.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_044.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_045.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_046.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_047.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_048.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_049.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_050.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_051.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_052.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_053.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_054.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_055.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_056.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_057.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_058.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_059.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_060.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_061.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_062.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_063.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_064.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_065.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_066.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_067.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_068.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_069.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_070.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_071.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_072.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_073.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_074.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_075.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_076.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_077.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_078.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_079.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_080.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_081.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_082.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_083.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_084.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_085.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_086.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_087.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_088.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_089.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_090.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_091.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_092.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_093.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_094.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_095.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_096.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_097.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_098.img', '/home/bertrand/mygit/nilearn/nilearn_data/spm_auditory/sub001/fM00223/fM00223_099.img']


Display the first functional image:



.. code-block:: python

    from nilearn.plotting import plot_stat_map, plot_anat, plot_img
    plot_img(subject_data.func[0])




.. image:: /auto_examples/01_tutorials/images/sphx_glr_plot_spm_auditory_001.png
    :align: center




Display the subject's anatomical image:



.. code-block:: python

    plot_anat(subject_data.anat)





.. image:: /auto_examples/01_tutorials/images/sphx_glr_plot_spm_auditory_002.png
    :align: center




Next, we concatenate all the 3D EPI image into a single 4D image:



.. code-block:: python


    from nilearn.image import concat_imgs
    fmri_img = concat_imgs(subject_data.func)







And we average all the EPI images in order to create a background
image that will be used to display the activations:



.. code-block:: python


    from nilearn import image
    mean_img = image.mean_img(fmri_img)







Specifying the experimental paradigm
------------------------------------

We must provide a description of the experiment, that is, define the
timing of the auditory stimulation and rest periods. According to
the documentation of the dataset, there were 16 42s blocks --- in
which 6 scans were acquired --- alternating between rest and
auditory stimulation, starting with rest. We use standard python
functions to create a pandas.DataFrame object that specifies the
timings:



.. code-block:: python


    import numpy as np
    tr = 7.
    slice_time_ref = 0.
    n_scans = 96
    epoch_duration = 6 * tr  # duration in seconds
    conditions = ['rest', 'active'] * 8
    n_blocks = len(conditions)
    duration = epoch_duration * np.ones(n_blocks)
    onset = np.linspace(0, (n_blocks - 1) * epoch_duration, n_blocks)

    import pandas as pd
    events = pd.DataFrame(
        {'onset': onset, 'duration': duration, 'trial_type': conditions})







The ``events`` object contains the information for the design:



.. code-block:: python

    print(events)






.. rst-class:: sphx-glr-script-out

 Out::

    onset  duration trial_type
    0     0.0      42.0       rest
    1    42.0      42.0     active
    2    84.0      42.0       rest
    3   126.0      42.0     active
    4   168.0      42.0       rest
    5   210.0      42.0     active
    6   252.0      42.0       rest
    7   294.0      42.0     active
    8   336.0      42.0       rest
    9   378.0      42.0     active
    10  420.0      42.0       rest
    11  462.0      42.0     active
    12  504.0      42.0       rest
    13  546.0      42.0     active
    14  588.0      42.0       rest
    15  630.0      42.0     active


Performing the GLM analysis
---------------------------

We need to construct a *design matrix* using the timing information
provided by the ``events`` object. The design matrix contains
regressors of interest as well as regressors of non-interest
modeling temporal drifts:



.. code-block:: python


    frame_times = np.linspace(0, (n_scans - 1) * tr, n_scans)
    drift_model = 'Cosine'
    period_cut = 4. * epoch_duration
    hrf_model = 'glover + derivative'







It is now time to create a ``FirstLevelModel`` object
and fit it to the 4D dataset (Fitting means that the coefficients of the
model are estimated to best approximate data)



.. code-block:: python


    from nistats.first_level_model import FirstLevelModel

    fmri_glm = FirstLevelModel(tr, slice_time_ref, noise_model='ar1',
                               standardize=False, hrf_model=hrf_model,
                               drift_model=drift_model, period_cut=period_cut)
    fmri_glm = fmri_glm.fit(fmri_img, events)







One can inspect the design matrix (rows represent time, and
columns contain the predictors):



.. code-block:: python


    from nistats.reporting import plot_design_matrix
    design_matrix = fmri_glm.design_matrices_[0]
    plot_design_matrix(design_matrix)
    plt.show()




.. image:: /auto_examples/01_tutorials/images/sphx_glr_plot_spm_auditory_003.png
    :align: center




The first column contains the expected reponse profile of regions which are
sensitive to the auditory stimulation.



.. code-block:: python



    plt.plot(design_matrix['active'])
    plt.xlabel('scan')
    plt.title('Expected Auditory Response')
    plt.show()





.. image:: /auto_examples/01_tutorials/images/sphx_glr_plot_spm_auditory_004.png
    :align: center




Detecting voxels with significant effects
-----------------------------------------

To access the estimated coefficients (Betas of the GLM model), we
created constrasts with a single '1' in each of the columns:



.. code-block:: python


    contrast_matrix = np.eye(design_matrix.shape[1])
    contrasts = dict([(column, contrast_matrix[i])
                      for i, column in enumerate(design_matrix.columns)])

    """
    contrasts::

      {
      'active':            array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]),
      'active_derivative': array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]),
      'constant':          array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]),
      'drift_1':           array([ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]),
      'drift_2':           array([ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]),
      'drift_3':           array([ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.]),
      'drift_4':           array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.]),
      'drift_5':           array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.]),
      'drift_6':           array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.]),
      'drift_7':           array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.]),
      'rest':              array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]),
      'rest_derivative':   array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])}
    """







We can then compare the two conditions 'active' and 'rest' by
generating the relevant contrast:



.. code-block:: python


    active_minus_rest = contrasts['active'] - contrasts['rest']

    eff_map = fmri_glm.compute_contrast(active_minus_rest,
                                        output_type='effect_size')

    z_map = fmri_glm.compute_contrast(active_minus_rest,
                                      output_type='z_score')







Plot thresholded z scores map



.. code-block:: python


    plot_stat_map(z_map, bg_img=mean_img, threshold=3.0,
                  display_mode='z', cut_coords=3, black_bg=True,
                  title='Active minus Rest (Z>3)')
    plt.show()




.. image:: /auto_examples/01_tutorials/images/sphx_glr_plot_spm_auditory_005.png
    :align: center




We can use ``nibabel.save`` to save the effect and zscore maps to the disk



.. code-block:: python


    import os
    outdir = 'results'
    if not os.path.exists(outdir):
        os.mkdir(outdir)

    import nibabel
    from os.path import join
    nibabel.save(z_map, join('results', 'active_vs_rest_z_map.nii'))
    nibabel.save(eff_map, join('results', 'active_vs_rest_eff_map.nii'))







Extract the signal from a voxel
 -------------------------------

We search for the voxel with the larger z-score and plot the signal
(warning: this is "double dipping")



.. code-block:: python



    # Find the coordinates of the peak

    from nibabel.affines import apply_affine
    values = z_map.get_data()
    coord_peaks = np.dstack(np.unravel_index(np.argsort(-values.ravel()),
                                             values.shape))[0, 0, :]
    coord_mm = apply_affine(z_map.affine, coord_peaks)







We create a masker for the voxel (allowing us to detrend the signal)
and extract the time course



.. code-block:: python


    from nilearn.input_data import NiftiSpheresMasker
    mask = NiftiSpheresMasker([coord_mm], radius=3,
                              detrend=True, standardize=True,
                              high_pass=None, low_pass=None, t_r=7.)
    sig = mask.fit_transform(fmri_img)







Let's plot the signal and the theoretical response



.. code-block:: python


    plt.plot(frame_times, sig, label='voxel %d %d %d' % tuple(coord_mm))
    plt.plot(design_matrix['active'], color='red', label='model')
    plt.xlabel('scan')
    plt.legend()
    plt.show()



.. image:: /auto_examples/01_tutorials/images/sphx_glr_plot_spm_auditory_006.png
    :align: center




**Total running time of the script:** ( 0 minutes  8.802 seconds)



.. container:: sphx-glr-footer


  .. container:: sphx-glr-download

     :download:`Download Python source code: plot_spm_auditory.py <plot_spm_auditory.py>`



  .. container:: sphx-glr-download

     :download:`Download Jupyter notebook: plot_spm_auditory.ipynb <plot_spm_auditory.ipynb>`

.. rst-class:: sphx-glr-signature

    `Generated by Sphinx-Gallery <https://sphinx-gallery.readthedocs.io>`_
